{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction Welcome to cuallee and thanks for using this amazing framework. None of this work could have been possible without the inspiration of pydeequ . So, thanks to the AWS folks for putting the research work together, and the references so that we could build on the shoulders of giants. This pure-python implementation of unit tests for your data, will help you define validations for your data using 3 concepts described below: Entities To better understand cuallee you will need to get familiar with the following 3 concepts: Check , Rule and ComputeInstruction . Entity Icon Description Check Use it to define a group of validations on a dataframe and report them as WARNING or ERROR . You can chain as many rules into a check , internally cuallee will make sure the same rule is not executed twice. Rule A rule represents the predicate you want to test on a single or multiple columns in a dataframe. A rule as a 4 attributes method : name of the predicate, column : the column in the dataframe, value : the value to compare against and coverage : the percentage of positive predicate necessary to set the status of the check to PASS . ComputeInstruction Are the implementation specific representations of the predicates in the rule . Because cuallee is a dataframe agnostic data quality framework, the implementation of the rules, rely in the creation of compute instructions passed to the specific dataframe of choice, including the following dataframe options: pandas , pyspark and snowpark In principle, the only interface you need to be familiar with is the Check as it is through this object that you can chain your validations and then directly through the validate method, execute validations on any DataFrame . Process Flow graph LR U((start)) --> A; A[Check] -.-> B(is_complete); A[Check] -.-> C(is_between); A[Check] -.-> D(is_on_weekday); B --> E{all rules?}; C --> E{all rules?}; D --> E{all rules?}; E --> F[/read dataframe/]; A -.-> G{want results?}; F --> G; G --> H(validate); H --> I([get results]) I --> K((end)) Installation cuallee is designed to work primarily with pyspark==3.3.0 and this is its only dependency. It uses the Observation API features in pyspark, to reduce the computation time for aggregations, and calculating summaries in one pass of the data frames being validated. pip # Latest pip install cuallee Check Validating data sets is about creating a Check and adding rules into it. You can choose from different types: numeric , date algebra , range of values , temporal , and many others. A Check provides a declarative interface to build a comprehensive validation on a dataframe as shown below: # Imports from cuallee import Check , CheckLevel from cuallee import dataframe as D # Check check = Check ( CheckLevel . WARNING , \"TaxiNYCheck\" ) # Data df = spark . read . parquet ( \"temp/taxi/*.parquet\" ) # Adding rules # ============= # All fields are filled [ check . is_complete ( name ) for name in df . columns ] # Verify taxi ride distance is positive [ check . is_greater_than ( name , 0 ) for name in D . numeric_fields ( df )] # Confirm that tips are not outliers [ check . is_less_than ( name , 1e4 ) for name in D . numeric_fields ( df )] # 70% of data is on weekdays [ check . is_on_weekday ( name , .7 ) for name in D . timestamp_fields ( df )] # Binary classification fields [ check . has_entropy ( name , 1.0 , 0.5 ) for name in D . numeric_fields ( df )] # Percentage of big tips [ check . is_between ( name , ( 1000 , 2000 )) for name in D . numeric_fields ( df )] # Confirm 22 years of data [ check . is_between ( name , ( \"2000-01-01\" , \"2022-12-31\" )) for name in D . timestamp_fields ( df )] # Validation check . validate ( df )","title":"Home"},{"location":"#introduction","text":"Welcome to cuallee and thanks for using this amazing framework. None of this work could have been possible without the inspiration of pydeequ . So, thanks to the AWS folks for putting the research work together, and the references so that we could build on the shoulders of giants. This pure-python implementation of unit tests for your data, will help you define validations for your data using 3 concepts described below:","title":"Introduction"},{"location":"#entities","text":"To better understand cuallee you will need to get familiar with the following 3 concepts: Check , Rule and ComputeInstruction . Entity Icon Description Check Use it to define a group of validations on a dataframe and report them as WARNING or ERROR . You can chain as many rules into a check , internally cuallee will make sure the same rule is not executed twice. Rule A rule represents the predicate you want to test on a single or multiple columns in a dataframe. A rule as a 4 attributes method : name of the predicate, column : the column in the dataframe, value : the value to compare against and coverage : the percentage of positive predicate necessary to set the status of the check to PASS . ComputeInstruction Are the implementation specific representations of the predicates in the rule . Because cuallee is a dataframe agnostic data quality framework, the implementation of the rules, rely in the creation of compute instructions passed to the specific dataframe of choice, including the following dataframe options: pandas , pyspark and snowpark In principle, the only interface you need to be familiar with is the Check as it is through this object that you can chain your validations and then directly through the validate method, execute validations on any DataFrame .","title":"Entities"},{"location":"#process-flow","text":"graph LR U((start)) --> A; A[Check] -.-> B(is_complete); A[Check] -.-> C(is_between); A[Check] -.-> D(is_on_weekday); B --> E{all rules?}; C --> E{all rules?}; D --> E{all rules?}; E --> F[/read dataframe/]; A -.-> G{want results?}; F --> G; G --> H(validate); H --> I([get results]) I --> K((end))","title":"Process Flow"},{"location":"#installation","text":"cuallee is designed to work primarily with pyspark==3.3.0 and this is its only dependency. It uses the Observation API features in pyspark, to reduce the computation time for aggregations, and calculating summaries in one pass of the data frames being validated.","title":"Installation"},{"location":"#pip","text":"# Latest pip install cuallee","title":"pip"},{"location":"#check","text":"Validating data sets is about creating a Check and adding rules into it. You can choose from different types: numeric , date algebra , range of values , temporal , and many others. A Check provides a declarative interface to build a comprehensive validation on a dataframe as shown below: # Imports from cuallee import Check , CheckLevel from cuallee import dataframe as D # Check check = Check ( CheckLevel . WARNING , \"TaxiNYCheck\" ) # Data df = spark . read . parquet ( \"temp/taxi/*.parquet\" ) # Adding rules # ============= # All fields are filled [ check . is_complete ( name ) for name in df . columns ] # Verify taxi ride distance is positive [ check . is_greater_than ( name , 0 ) for name in D . numeric_fields ( df )] # Confirm that tips are not outliers [ check . is_less_than ( name , 1e4 ) for name in D . numeric_fields ( df )] # 70% of data is on weekdays [ check . is_on_weekday ( name , .7 ) for name in D . timestamp_fields ( df )] # Binary classification fields [ check . has_entropy ( name , 1.0 , 0.5 ) for name in D . numeric_fields ( df )] # Percentage of big tips [ check . is_between ( name , ( 1000 , 2000 )) for name in D . numeric_fields ( df )] # Confirm 22 years of data [ check . is_between ( name , ( \"2000-01-01\" , \"2022-12-31\" )) for name in D . timestamp_fields ( df )] # Validation check . validate ( df )","title":"Check"},{"location":"advanced/","text":"Advanced Checks satisfies This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . satisfies simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_entropy This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_entropy simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_weekday_continuity This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_weekday_continuity simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"Advanced"},{"location":"advanced/#advanced-checks","text":"","title":"Advanced Checks"},{"location":"advanced/#satisfies","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . satisfies simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"satisfies"},{"location":"advanced/#has_entropy","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_entropy simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_entropy"},{"location":"advanced/#has_weekday_continuity","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_weekday_continuity simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_weekday_continuity"},{"location":"dependencies/","text":"Dependencies","title":"Dependencies"},{"location":"dependencies/#dependencies","text":"","title":"Dependencies"},{"location":"installation/","text":"Installation","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"new-check/","text":"New Check","title":"New Checks"},{"location":"new-check/#new-check","text":"","title":"New Check"},{"location":"validate/","text":"validate","title":"Validate"},{"location":"validate/#validate","text":"","title":"validate"},{"location":"pandas/","text":"Check pandas is_complete This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_complete simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ are_complete This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . are_complete simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_unique This check verifies that the number of distinct values for a column is equal to the number of rows of the dataframe. is_unique simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ are_unique This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . are_unique simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_greater_than This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_greater_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_greater_or_equal_than This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_greater_or_equal_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_less_than This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_less_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_less_or_equal_than This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_less_or_equal_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_equal_than This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_equal_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_pattern This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_pattern simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_min This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_min simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_max This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_max simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_std This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_std simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_mean This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_mean simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_between This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_between simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_contained_in This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_contained_in simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_in This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_in simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_weekday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_weekday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_weekend This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_weekend simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_monday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_monday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_tuesday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_tuesday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_wednesday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_wednesday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_thursday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_thursday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_friday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_friday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_saturday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_saturday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_sunday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_sunday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_schedule This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_schedule simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_percentile This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_percentile simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_max_by This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_max_by simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_min_by This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_min_by simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_correlation This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_correlation simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"Check"},{"location":"pandas/#check","text":"","title":"Check"},{"location":"pandas/#pandas","text":"","title":"pandas"},{"location":"pandas/#is_complete","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_complete simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_complete"},{"location":"pandas/#are_complete","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . are_complete simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"are_complete"},{"location":"pandas/#is_unique","text":"This check verifies that the number of distinct values for a column is equal to the number of rows of the dataframe. is_unique simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_unique"},{"location":"pandas/#are_unique","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . are_unique simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"are_unique"},{"location":"pandas/#is_greater_than","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_greater_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_greater_than"},{"location":"pandas/#is_greater_or_equal_than","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_greater_or_equal_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_greater_or_equal_than"},{"location":"pandas/#is_less_than","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_less_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_less_than"},{"location":"pandas/#is_less_or_equal_than","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_less_or_equal_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_less_or_equal_than"},{"location":"pandas/#is_equal_than","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_equal_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_equal_than"},{"location":"pandas/#has_pattern","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_pattern simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_pattern"},{"location":"pandas/#has_min","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_min simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_min"},{"location":"pandas/#has_max","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_max simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_max"},{"location":"pandas/#has_std","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_std simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_std"},{"location":"pandas/#has_mean","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_mean simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_mean"},{"location":"pandas/#is_between","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_between simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_between"},{"location":"pandas/#is_contained_in","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_contained_in simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_contained_in"},{"location":"pandas/#is_in","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_in simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_in"},{"location":"pandas/#is_on_weekday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_weekday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_weekday"},{"location":"pandas/#is_on_weekend","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_weekend simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_weekend"},{"location":"pandas/#is_on_monday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_monday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_monday"},{"location":"pandas/#is_on_tuesday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_tuesday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_tuesday"},{"location":"pandas/#is_on_wednesday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_wednesday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_wednesday"},{"location":"pandas/#is_on_thursday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_thursday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_thursday"},{"location":"pandas/#is_on_friday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_friday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_friday"},{"location":"pandas/#is_on_saturday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_saturday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_saturday"},{"location":"pandas/#is_on_sunday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_sunday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_sunday"},{"location":"pandas/#is_on_schedule","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_schedule simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_schedule"},{"location":"pandas/#has_percentile","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_percentile simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_percentile"},{"location":"pandas/#has_max_by","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_max_by simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_max_by"},{"location":"pandas/#has_min_by","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_min_by simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_min_by"},{"location":"pandas/#has_correlation","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_correlation simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_correlation"},{"location":"pyspark/","text":"Check pyspark is_complete This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_complete simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ are_complete This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . are_complete simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_unique This check verifies that the number of distinct values for a column is equal to the number of rows of the dataframe. is_unique simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ are_unique This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . are_unique simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_greater_than This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_greater_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_greater_or_equal_than This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_greater_or_equal_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_less_than This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_less_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_less_or_equal_than This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_less_or_equal_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_equal_than This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_equal_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_pattern This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_pattern simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_min This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_min simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_max This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_max simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_std This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_std simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_mean This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_mean simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_between This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_between simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_contained_in This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_contained_in simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_in This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_in simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_weekday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_weekday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_weekend This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_weekend simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_monday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_monday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_tuesday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_tuesday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_wednesday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_wednesday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_thursday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_thursday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_friday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_friday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_saturday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_saturday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_sunday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_sunday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_schedule This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_schedule simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_percentile This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_percentile simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_max_by This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_max_by simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_min_by This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_min_by simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_correlation This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_correlation simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"Check"},{"location":"pyspark/#check","text":"","title":"Check"},{"location":"pyspark/#pyspark","text":"","title":"pyspark"},{"location":"pyspark/#is_complete","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_complete simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_complete"},{"location":"pyspark/#are_complete","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . are_complete simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"are_complete"},{"location":"pyspark/#is_unique","text":"This check verifies that the number of distinct values for a column is equal to the number of rows of the dataframe. is_unique simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_unique"},{"location":"pyspark/#are_unique","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . are_unique simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"are_unique"},{"location":"pyspark/#is_greater_than","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_greater_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_greater_than"},{"location":"pyspark/#is_greater_or_equal_than","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_greater_or_equal_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_greater_or_equal_than"},{"location":"pyspark/#is_less_than","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_less_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_less_than"},{"location":"pyspark/#is_less_or_equal_than","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_less_or_equal_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_less_or_equal_than"},{"location":"pyspark/#is_equal_than","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_equal_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_equal_than"},{"location":"pyspark/#has_pattern","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_pattern simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_pattern"},{"location":"pyspark/#has_min","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_min simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_min"},{"location":"pyspark/#has_max","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_max simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_max"},{"location":"pyspark/#has_std","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_std simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_std"},{"location":"pyspark/#has_mean","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_mean simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_mean"},{"location":"pyspark/#is_between","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_between simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_between"},{"location":"pyspark/#is_contained_in","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_contained_in simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_contained_in"},{"location":"pyspark/#is_in","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_in simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_in"},{"location":"pyspark/#is_on_weekday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_weekday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_weekday"},{"location":"pyspark/#is_on_weekend","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_weekend simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_weekend"},{"location":"pyspark/#is_on_monday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_monday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_monday"},{"location":"pyspark/#is_on_tuesday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_tuesday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_tuesday"},{"location":"pyspark/#is_on_wednesday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_wednesday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_wednesday"},{"location":"pyspark/#is_on_thursday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_thursday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_thursday"},{"location":"pyspark/#is_on_friday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_friday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_friday"},{"location":"pyspark/#is_on_saturday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_saturday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_saturday"},{"location":"pyspark/#is_on_sunday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_sunday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_sunday"},{"location":"pyspark/#is_on_schedule","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_schedule simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_schedule"},{"location":"pyspark/#has_percentile","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_percentile simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_percentile"},{"location":"pyspark/#has_max_by","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_max_by simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_max_by"},{"location":"pyspark/#has_min_by","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_min_by simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_min_by"},{"location":"pyspark/#has_correlation","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_correlation simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_correlation"},{"location":"pyspark/observation/","text":"","title":"About observation"},{"location":"snowpark/","text":"Check snowpark is_complete This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_complete simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ are_complete This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . are_complete simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_unique This check verifies that the number of distinct values for a column is equal to the number of rows of the dataframe. is_unique simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ are_unique This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . are_unique simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_greater_than This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_greater_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_greater_or_equal_than This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_greater_or_equal_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_less_than This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_less_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_less_or_equal_than This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_less_or_equal_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_equal_than This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_equal_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_pattern This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_pattern simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_min This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_min simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_max This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_max simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_std This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_std simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_mean This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_mean simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_between This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_between simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_contained_in This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_contained_in simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_in This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_in simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_weekday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_weekday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_weekend This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_weekend simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_monday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_monday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_tuesday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_tuesday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_wednesday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_wednesday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_thursday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_thursday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_friday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_friday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_saturday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_saturday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_sunday This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_sunday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ is_on_schedule This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_schedule simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_percentile This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_percentile simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_max_by This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_max_by simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_min_by This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_min_by simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ has_correlation This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_correlation simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"Check"},{"location":"snowpark/#check","text":"","title":"Check"},{"location":"snowpark/#snowpark","text":"","title":"snowpark"},{"location":"snowpark/#is_complete","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_complete simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_complete"},{"location":"snowpark/#are_complete","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . are_complete simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"are_complete"},{"location":"snowpark/#is_unique","text":"This check verifies that the number of distinct values for a column is equal to the number of rows of the dataframe. is_unique simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_unique"},{"location":"snowpark/#are_unique","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . are_unique simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"are_unique"},{"location":"snowpark/#is_greater_than","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_greater_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_greater_than"},{"location":"snowpark/#is_greater_or_equal_than","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_greater_or_equal_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_greater_or_equal_than"},{"location":"snowpark/#is_less_than","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_less_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_less_than"},{"location":"snowpark/#is_less_or_equal_than","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_less_or_equal_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_less_or_equal_than"},{"location":"snowpark/#is_equal_than","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_equal_than simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_equal_than"},{"location":"snowpark/#has_pattern","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_pattern simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_pattern"},{"location":"snowpark/#has_min","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_min simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_min"},{"location":"snowpark/#has_max","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_max simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_max"},{"location":"snowpark/#has_std","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_std simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_std"},{"location":"snowpark/#has_mean","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_mean simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_mean"},{"location":"snowpark/#is_between","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_between simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_between"},{"location":"snowpark/#is_contained_in","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_contained_in simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_contained_in"},{"location":"snowpark/#is_in","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_in simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_in"},{"location":"snowpark/#is_on_weekday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_weekday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_weekday"},{"location":"snowpark/#is_on_weekend","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_weekend simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_weekend"},{"location":"snowpark/#is_on_monday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_monday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_monday"},{"location":"snowpark/#is_on_tuesday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_tuesday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_tuesday"},{"location":"snowpark/#is_on_wednesday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_wednesday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_wednesday"},{"location":"snowpark/#is_on_thursday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_thursday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_thursday"},{"location":"snowpark/#is_on_friday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_friday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_friday"},{"location":"snowpark/#is_on_saturday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_saturday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_saturday"},{"location":"snowpark/#is_on_sunday","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_sunday simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_sunday"},{"location":"snowpark/#is_on_schedule","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . is_on_schedule simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"is_on_schedule"},{"location":"snowpark/#has_percentile","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_percentile simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_percentile"},{"location":"snowpark/#has_max_by","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_max_by simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_max_by"},{"location":"snowpark/#has_min_by","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_min_by simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_min_by"},{"location":"snowpark/#has_correlation","text":"This check is the most popular. It validates the completeness attribute of a data set. It confirms that all fields contain values different of null . has_correlation simple coverage from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"CompletePredicate\" ) check . is_complete ( \"id\" ) # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |1.0 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ from cuallee import Check , CheckLevel df = spark . range ( 10 ) check = Check ( CheckLevel . WARNING , \"IsComplete\" ) check . is_complete ( \"id\" , .5 ) # Only 50% coverage # Validate check . validate ( spark , df ) . show ( truncate = False ) Result: +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |id |timestamp |check |level |column|rule |value|rows|violations|pass_rate|pass_threshold|metadata|status| +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+ |1 |2022-10-09 23:45:10|CompletePredicate|WARNING|id |is_complete|N/A |10 |0 |1.0 |0.5 |{} |PASS | +---+-------------------+-----------------+-------+------+-----------+-----+----+----------+---------+--------------+--------+------+","title":"has_correlation"}]}